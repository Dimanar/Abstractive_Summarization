# TO DO

# 1. Write Positional Embedding (Done)
# 2. Write Word Embedding (Done)
# 3. Write Attention (Done)
# 4. Write Multi-Head Attention (Done)
# 5. Write Normalization (Done)
# 6. Write Feed Forward Network (Done)
# 7. Write Encoder
# 8. Write Decoder
# 9. Write Whole Transformer

# 7. Write DataLoader
# 8.1 Write Optimizer
# 8.2 Write Pipeline
# 9. Prepare model for additional training

# 10. Save model/pipeline


## ADDITIONAL TASK (for team)
#
#  Write notebook with explanation of transformers and
#  show some results in picture
